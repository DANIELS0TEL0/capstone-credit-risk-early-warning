{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42cc5a78",
   "metadata": {},
   "source": [
    "# 03 â€” Modeling & Evaluation (Early Default)\n",
    "#\n",
    "# **Objective**\n",
    "# Train and evaluate predictive models for `early_default` using a realistic temporal split.\n",
    "#\n",
    "# **Datasets**\n",
    "# - **BASE**: origination-only features (no macro)\n",
    "# - **MACRO**: BASE + monthly macroeconomic indicators (UNRATE, FEDFUNDS, CPI)\n",
    "# \n",
    "# **Models**\n",
    "# 1) Logistic Regression (interpretable baseline)\n",
    "# 2) XGBoost (nonlinear model with class-imbalance handling)\n",
    "#\n",
    "# **Evaluation**\n",
    "# - ROC-AUC (ranking ability)\n",
    "# - PR-AUC (rare-event performance)\n",
    "# - Threshold policy (recall / review-rate tradeoffs)\n",
    "# - Calibration check (are probabilities meaningful as PDs?)\n",
    "# - Lift (risk concentration in high-score segments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71390897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c2078",
   "metadata": {},
   "source": [
    "# 1) Load Processed Dataset (from Notebook 02)\n",
    "# This dataset is already:\n",
    "# - target-defined (`early_default`)\n",
    "# - filtered for observability (>= 6 months observable OR early default within 6 months)\n",
    "# - imputed (median for numeric; \"Unknown\" for categorical)\n",
    "# - one-hot encoded\n",
    "#\n",
    "# `issue_d` is retained only for temporal splitting, not as a predictive feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "750a5979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base shape: (1976721, 136)\n",
      "Macro shape: (1976721, 139)\n"
     ]
    }
   ],
   "source": [
    "df_base = pd.read_csv(\n",
    "    \"../data/processed/early_default_modeling_dataset.csv\",\n",
    "    parse_dates=[\"issue_d\"]\n",
    ")\n",
    "\n",
    "df_macro = pd.read_csv(\n",
    "    \"../data/processed/early_default_modeling_dataset_macro.csv\",\n",
    "    parse_dates=[\"issue_d\"]\n",
    ")\n",
    "\n",
    "print(\"Base shape:\", df_base.shape)\n",
    "print(\"Macro shape:\", df_macro.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de90b79",
   "metadata": {},
   "source": [
    "# 2) Temporal Train/Test Split\n",
    "# To simulate real deployment:\n",
    "# - Train on older loans (pre-2015)\n",
    "# - Test on newer loans (2015+)\n",
    "# This avoids look-ahead bias and provides realistic generalization testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eaafb893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train default rate (base): 0.018583582566276607\n",
      "Test default rate (base): 0.023350342115899526\n",
      "Train default rate (macro): 0.018583582566276607\n",
      "Test default rate (macro): 0.023350342115899526\n"
     ]
    }
   ],
   "source": [
    "def time_split(df, split_date=\"2015-01-01\"):\n",
    "    train = df[df[\"issue_d\"] < split_date].copy()\n",
    "    test  = df[df[\"issue_d\"] >= split_date].copy()\n",
    "\n",
    "    X_train = train.drop(columns=[\"early_default\", \"issue_d\"])\n",
    "    y_train = train[\"early_default\"]\n",
    "\n",
    "    X_test = test.drop(columns=[\"early_default\", \"issue_d\"])\n",
    "    y_test = test[\"early_default\"]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "Xtr_base, Xte_base, ytr, yte = time_split(df_base)\n",
    "Xtr_macro, Xte_macro, ytr2, yte2 = time_split(df_macro)\n",
    "\n",
    "print(\"Train default rate (base):\", ytr.mean())\n",
    "print(\"Test default rate (base):\", yte.mean())\n",
    "print(\"Train default rate (macro):\", ytr2.mean())\n",
    "print(\"Test default rate (macro):\", yte2.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aaec7a",
   "metadata": {},
   "source": [
    "# Sanity check: NaNs\n",
    "# Since Notebook 02 performed imputation, we expect no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791cd29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs base train: 0\n",
      "NaNs base test: 0\n",
      "NaNs macro train: 0\n",
      "NaNs macro test: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"NaNs base train:\", Xtr_base.isna().sum().sum())\n",
    "print(\"NaNs base test:\", Xte_base.isna().sum().sum())\n",
    "print(\"NaNs macro train:\", Xtr_macro.isna().sum().sum())\n",
    "print(\"NaNs macro test:\", Xte_macro.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdab904",
   "metadata": {},
   "source": [
    "# 3) Utility: Metric evaluation (ROC-AUC and PR-AUC)\n",
    "# PR-AUC is particularly important because early default is a rare event.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12f1e520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "\n",
    "def eval_probs(y_true, y_proba, label):\n",
    "    roc = roc_auc_score(y_true, y_proba)\n",
    "    p, r, _ = precision_recall_curve(y_true, y_proba)\n",
    "    pr = auc(r, p)\n",
    "    print(f\"{label} ROC-AUC: {roc:.4f}\")\n",
    "    print(f\"{label} PR-AUC : {pr:.4f}\")\n",
    "    return roc, pr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23626aa",
   "metadata": {},
   "source": [
    "# 4) Logistic Regression (Baseline)\n",
    "# We standardize features and use `class_weight='balanced'` due to class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da40295a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic (BASE) ROC-AUC: 0.7188\n",
      "Logistic (BASE) PR-AUC : 0.0586\n",
      "Logistic (MACRO) ROC-AUC: 0.7123\n",
      "Logistic (MACRO) PR-AUC : 0.0577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.7122670565694902), np.float64(0.0577295642505537))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_logistic(X_train, y_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    Xtr_s = scaler.fit_transform(X_train)\n",
    "    Xte_s = scaler.transform(X_test)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "    model.fit(Xtr_s, y_train)\n",
    "\n",
    "    proba = model.predict_proba(Xte_s)[:, 1]\n",
    "    return model, scaler, proba\n",
    "\n",
    "log_base, sc_base, proba_log_base = train_logistic(Xtr_base, ytr, Xte_base)\n",
    "eval_probs(yte, proba_log_base, \"Logistic (BASE)\")\n",
    "\n",
    "log_macro, sc_macro, proba_log_macro = train_logistic(Xtr_macro, ytr2, Xte_macro)\n",
    "eval_probs(yte2, proba_log_macro, \"Logistic (MACRO)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c828cb9e",
   "metadata": {},
   "source": [
    "# 5) XGBoost (Advanced)\n",
    "# XGBoost is strict about feature names. Some one-hot encoded columns can contain special characters\n",
    "# that XGBoost rejects (e.g., `[`, `]`, `<`). We sanitize column names and align train/test columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbabeb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad cols remaining: 0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_xgb_columns(X_train, X_test):\n",
    "    # Ensure column names are strings + remove illegal characters\n",
    "    X_train = X_train.copy()\n",
    "    X_test = X_test.copy()\n",
    "\n",
    "    X_train.columns = [re.sub(r\"[^A-Za-z0-9_]+\", \"_\", str(c)) for c in X_train.columns]\n",
    "    X_test.columns  = [re.sub(r\"[^A-Za-z0-9_]+\", \"_\", str(c)) for c in X_test.columns]\n",
    "\n",
    "    # Ensure same columns and same order\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "Xtr_base_xgb, Xte_base_xgb = clean_xgb_columns(Xtr_base, Xte_base)\n",
    "Xtr_macro_xgb, Xte_macro_xgb = clean_xgb_columns(Xtr_macro, Xte_macro)\n",
    "\n",
    "# Quick sanity: no illegal characters remain\n",
    "bad_cols = [c for c in Xtr_base_xgb.columns if any(ch in c for ch in [\"[\", \"]\", \"<\"])]\n",
    "print(\"Bad cols remaining:\", len(bad_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "285535de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost (BASE) ROC-AUC: 0.7187\n",
      "XGBoost (BASE) PR-AUC : 0.0601\n",
      "XGBoost (MACRO) ROC-AUC: 0.6808\n",
      "XGBoost (MACRO) PR-AUC : 0.0483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(np.float64(0.6808290574625906), np.float64(0.04826998178318193))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_xgb(X_train, y_train, X_test):\n",
    "    # scale_pos_weight = (# negative) / (# positive)\n",
    "    neg, pos = np.bincount(y_train)\n",
    "    spw = neg / pos\n",
    "\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=300,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        scale_pos_weight=spw,\n",
    "        eval_metric=\"logloss\"\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    proba = model.predict_proba(X_test)[:, 1]\n",
    "    return model, spw, proba\n",
    "\n",
    "xgb_base, spw_base, proba_xgb_base = train_xgb(Xtr_base_xgb, ytr, Xte_base_xgb)\n",
    "eval_probs(yte, proba_xgb_base, \"XGBoost (BASE)\")\n",
    "\n",
    "xgb_macro, spw_macro, proba_xgb_macro = train_xgb(Xtr_macro_xgb, ytr2, Xte_macro_xgb)\n",
    "eval_probs(yte2, proba_xgb_macro, \"XGBoost (MACRO)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411ece6",
   "metadata": {},
   "source": [
    "# 6) Model Selection Decision\n",
    "# Based on the results, macro variables reduced performance (both ROC-AUC and PR-AUC).\n",
    "# Therefore, we proceed with **XGBoost (BASE)** as the final operational model.\n",
    "#\n",
    "# Next we will:\n",
    "# - choose a threshold based on an operational policy (e.g., >=80% recall)\n",
    "# - report review rate and confusion matrix\n",
    "# - check calibration and lift\n",
    "# - evaluate review-capacity scenarios (top 10/20/30%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68b6c59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model chosen: XGBoost (BASE)\n",
      "Final test default rate: 0.023350342115899526\n"
     ]
    }
   ],
   "source": [
    "# Final model artifacts used downstream:\n",
    "y_final = yte\n",
    "proba_final = proba_xgb_base\n",
    "\n",
    "print(\"Final model chosen: XGBoost (BASE)\")\n",
    "print(\"Final test default rate:\", y_final.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba06b80",
   "metadata": {},
   "source": [
    "# 7) Save trained artifacts\n",
    "# We save:\n",
    "# - base scaler + logistic (baseline)\n",
    "# - base xgboost (final model)\n",
    "# - policy threshold later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92a9a3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: models/scaler_base.joblib, models/log_model_base.joblib, models/xgb_model_base.joblib\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(sc_base, \"../models/scaler_base.joblib\")\n",
    "joblib.dump(log_base, \"../models/log_model_base.joblib\")\n",
    "joblib.dump(xgb_base, \"../models/xgb_model_base.joblib\")\n",
    "\n",
    "print(\"Saved: models/scaler_base.joblib, models/log_model_base.joblib, models/xgb_model_base.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cd45d2",
   "metadata": {},
   "source": [
    "# 8) Threshold Optimization (Reference)\n",
    "# We evaluate thresholds from 0.01 to 0.50.\n",
    "# \n",
    "# Note:\n",
    "# - F1 is included for reference only.\n",
    "# - In credit early-warning, threshold selection is usually policy-driven (recall vs review capacity).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "985079e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top thresholds by F1:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "threshold",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "762ec0bc-8db7-46cf-a816-2bec14387663",
       "rows": [
        [
         "49",
         "0.5",
         "0.04894281790821966",
         "0.5730050696019943",
         "0.09018274149715341"
        ],
        [
         "48",
         "0.49",
         "0.04799825262334022",
         "0.5908747164103857",
         "0.08878432890940809"
        ],
        [
         "47",
         "0.48000000000000004",
         "0.047018050353724514",
         "0.6075960003361062",
         "0.08728190086485246"
        ],
        [
         "46",
         "0.47000000000000003",
         "0.046097559966743744",
         "0.6242892754110299",
         "0.08585554128203597"
        ],
        [
         "45",
         "0.46",
         "0.04517235858808462",
         "0.6401422849620481",
         "0.08438966572756779"
        ],
        [
         "44",
         "0.45",
         "0.044276805243360653",
         "0.6554351174971291",
         "0.08295006017167093"
        ],
        [
         "43",
         "0.44",
         "0.0434317326440847",
         "0.6706719323306165",
         "0.08158043568620237"
        ],
        [
         "42",
         "0.43",
         "0.04256415254591275",
         "0.6849844550878077",
         "0.08014799982302945"
        ],
        [
         "41",
         "0.42000000000000004",
         "0.041805421815652916",
         "0.700417331876873",
         "0.07890149381822083"
        ],
        [
         "40",
         "0.41000000000000003",
         "0.04110100123184001",
         "0.7158502086659384",
         "0.07773859113631629"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.048943</td>\n",
       "      <td>0.573005</td>\n",
       "      <td>0.090183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.49</td>\n",
       "      <td>0.047998</td>\n",
       "      <td>0.590875</td>\n",
       "      <td>0.088784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.047018</td>\n",
       "      <td>0.607596</td>\n",
       "      <td>0.087282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.046098</td>\n",
       "      <td>0.624289</td>\n",
       "      <td>0.085856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.045172</td>\n",
       "      <td>0.640142</td>\n",
       "      <td>0.084390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.044277</td>\n",
       "      <td>0.655435</td>\n",
       "      <td>0.082950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.043432</td>\n",
       "      <td>0.670672</td>\n",
       "      <td>0.081580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.042564</td>\n",
       "      <td>0.684984</td>\n",
       "      <td>0.080148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.041805</td>\n",
       "      <td>0.700417</td>\n",
       "      <td>0.078901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.041101</td>\n",
       "      <td>0.715850</td>\n",
       "      <td>0.077739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  precision    recall        f1\n",
       "49       0.50   0.048943  0.573005  0.090183\n",
       "48       0.49   0.047998  0.590875  0.088784\n",
       "47       0.48   0.047018  0.607596  0.087282\n",
       "46       0.47   0.046098  0.624289  0.085856\n",
       "45       0.46   0.045172  0.640142  0.084390\n",
       "44       0.45   0.044277  0.655435  0.082950\n",
       "43       0.44   0.043432  0.670672  0.081580\n",
       "42       0.43   0.042564  0.684984  0.080148\n",
       "41       0.42   0.041805  0.700417  0.078901\n",
       "40       0.41   0.041101  0.715850  0.077739"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "thresholds = np.arange(0.01, 0.51, 0.01)\n",
    "precisions, recalls, f1s = [], [], []\n",
    "\n",
    "for t in thresholds:\n",
    "    y_pred = (proba_final >= t).astype(int)\n",
    "    precisions.append(precision_score(y_final, y_pred))\n",
    "    recalls.append(recall_score(y_final, y_pred))\n",
    "    f1s.append(f1_score(y_final, y_pred))\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"threshold\": thresholds,\n",
    "    \"precision\": precisions,\n",
    "    \"recall\": recalls,\n",
    "    \"f1\": f1s\n",
    "}).sort_values(\"f1\", ascending=False)\n",
    "\n",
    "print(\"Top thresholds by F1:\")\n",
    "display(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e410c",
   "metadata": {},
   "source": [
    "# 9) Policy Threshold: Target Recall (Operational Early Warning)\n",
    "# Early warning systems often prioritize catching defaults (high recall).\n",
    "# We choose the threshold that achieves at least 80% recall with the best precision available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a22672f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "threshold",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "8cc39079-b65c-4464-84f8-92b5fee332e2",
       "rows": [
        [
         "33",
         "0.34",
         "0.036373869912407215",
         "0.8109122482704535",
         "0.06962468992700133"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>threshold</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.036374</td>\n",
       "      <td>0.810912</td>\n",
       "      <td>0.069625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    threshold  precision    recall        f1\n",
       "33       0.34   0.036374  0.810912  0.069625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy threshold (>=80% recall): 0.34\n",
      "Saved: models/policy_threshold.joblib\n"
     ]
    }
   ],
   "source": [
    "target_recall = 0.80\n",
    "eligible = results[results[\"recall\"] >= target_recall]\n",
    "best_policy = eligible.sort_values(\"precision\", ascending=False).head(1)\n",
    "display(best_policy)\n",
    "\n",
    "policy_threshold = float(best_policy[\"threshold\"].iloc[0])\n",
    "print(\"Policy threshold (>=80% recall):\", policy_threshold)\n",
    "\n",
    "joblib.dump({\"policy_threshold\": policy_threshold}, \"../models/policy_threshold.joblib\")\n",
    "print(\"Saved: models/policy_threshold.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab345db",
   "metadata": {},
   "source": [
    "# 10) Performance at Policy Threshold\n",
    "# We report:\n",
    "# - review/flag rate (how many loans get flagged)\n",
    "# - confusion matrix\n",
    "# - classification report (precision/recall/F1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7826e22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy threshold: 0.34\n",
      "Flag / review rate: 0.5205681569952924\n",
      "Confusion Matrix:\n",
      " [[726307 767004]\n",
      " [  6751  28952]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.49      0.65   1493311\n",
      "           1       0.04      0.81      0.07     35703\n",
      "\n",
      "    accuracy                           0.49   1529014\n",
      "   macro avg       0.51      0.65      0.36   1529014\n",
      "weighted avg       0.97      0.49      0.64   1529014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_pred_policy = (proba_final >= policy_threshold).astype(int)\n",
    "flag_rate = y_pred_policy.mean()\n",
    "\n",
    "print(\"Policy threshold:\", policy_threshold)\n",
    "print(\"Flag / review rate:\", flag_rate)\n",
    "\n",
    "cm = confusion_matrix(y_final, y_pred_policy)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_final, y_pred_policy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3352d2c1",
   "metadata": {},
   "source": [
    "# 11) Calibration Check (Deciles)\n",
    "# Calibration compares predicted probabilities vs observed event rates.\n",
    "# We bin loans into risk deciles and compute:\n",
    "# - avg_pred: mean predicted probability in bin\n",
    "# - event_rate: observed default rate in bin\n",
    "# - count: number of loans in bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a27b380b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bdgsg\\AppData\\Local\\Temp\\ipykernel_8956\\3118397024.py:4: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  cal_summary = cal.groupby(\"bin\").agg(\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "bin",
         "rawType": "category",
         "type": "unknown"
        },
        {
         "name": "avg_pred",
         "rawType": "float32",
         "type": "float"
        },
        {
         "name": "event_rate",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "895aba94-f9b8-4b87-a5e2-1c30c4af1120",
       "rows": [
        [
         "0",
         "(0.0073999999999999995, 0.128]",
         "0.090002544",
         "0.003642856208551883",
         "152902"
        ],
        [
         "1",
         "(0.128, 0.19]",
         "0.15957949",
         "0.006422456360651664",
         "152901"
        ],
        [
         "2",
         "(0.19, 0.243]",
         "0.21675509",
         "0.00981026938999745",
         "152901"
        ],
        [
         "3",
         "(0.243, 0.295]",
         "0.2688971",
         "0.012099253116375195",
         "152902"
        ],
        [
         "4",
         "(0.295, 0.352]",
         "0.32319453",
         "0.015637569407655934",
         "152901"
        ],
        [
         "5",
         "(0.352, 0.414]",
         "0.38283506",
         "0.02026801655973473",
         "152901"
        ],
        [
         "6",
         "(0.414, 0.481]",
         "0.44731757",
         "0.02429660828504532",
         "152902"
        ],
        [
         "7",
         "(0.481, 0.554]",
         "0.5171024",
         "0.031379781688805175",
         "152901"
        ],
        [
         "8",
         "(0.554, 0.642]",
         "0.5961561",
         "0.0406472161725561",
         "152901"
        ],
        [
         "9",
         "(0.642, 0.927]",
         "0.7152811",
         "0.06929928974114138",
         "152902"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>avg_pred</th>\n",
       "      <th>event_rate</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.0073999999999999995, 0.128]</td>\n",
       "      <td>0.090003</td>\n",
       "      <td>0.003643</td>\n",
       "      <td>152902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.128, 0.19]</td>\n",
       "      <td>0.159579</td>\n",
       "      <td>0.006422</td>\n",
       "      <td>152901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.19, 0.243]</td>\n",
       "      <td>0.216755</td>\n",
       "      <td>0.009810</td>\n",
       "      <td>152901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.243, 0.295]</td>\n",
       "      <td>0.268897</td>\n",
       "      <td>0.012099</td>\n",
       "      <td>152902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.295, 0.352]</td>\n",
       "      <td>0.323195</td>\n",
       "      <td>0.015638</td>\n",
       "      <td>152901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.352, 0.414]</td>\n",
       "      <td>0.382835</td>\n",
       "      <td>0.020268</td>\n",
       "      <td>152901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.414, 0.481]</td>\n",
       "      <td>0.447318</td>\n",
       "      <td>0.024297</td>\n",
       "      <td>152902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.481, 0.554]</td>\n",
       "      <td>0.517102</td>\n",
       "      <td>0.031380</td>\n",
       "      <td>152901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.554, 0.642]</td>\n",
       "      <td>0.596156</td>\n",
       "      <td>0.040647</td>\n",
       "      <td>152901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.642, 0.927]</td>\n",
       "      <td>0.715281</td>\n",
       "      <td>0.069299</td>\n",
       "      <td>152902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              bin  avg_pred  event_rate   count\n",
       "0  (0.0073999999999999995, 0.128]  0.090003    0.003643  152902\n",
       "1                   (0.128, 0.19]  0.159579    0.006422  152901\n",
       "2                   (0.19, 0.243]  0.216755    0.009810  152901\n",
       "3                  (0.243, 0.295]  0.268897    0.012099  152902\n",
       "4                  (0.295, 0.352]  0.323195    0.015638  152901\n",
       "5                  (0.352, 0.414]  0.382835    0.020268  152901\n",
       "6                  (0.414, 0.481]  0.447318    0.024297  152902\n",
       "7                  (0.481, 0.554]  0.517102    0.031380  152901\n",
       "8                  (0.554, 0.642]  0.596156    0.040647  152901\n",
       "9                  (0.642, 0.927]  0.715281    0.069299  152902"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cal = pd.DataFrame({\"p\": proba_final, \"y\": y_final.values})\n",
    "cal[\"bin\"] = pd.qcut(cal[\"p\"], 10, duplicates=\"drop\")\n",
    "\n",
    "cal_summary = cal.groupby(\"bin\").agg(\n",
    "    avg_pred=(\"p\", \"mean\"),\n",
    "    event_rate=(\"y\", \"mean\"),\n",
    "    count=(\"y\", \"size\")\n",
    ").reset_index()\n",
    "\n",
    "display(cal_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1eceb7",
   "metadata": {},
   "source": [
    "# 12) Lift (Risk Concentration)\n",
    "# Lift compares default rate in the highest-risk decile to the overall default rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8018f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall default rate: 0.023350342115899526\n",
      "Top decile default rate: 0.06929928974114138\n",
      "Top decile lift: 2.967806184473617\n"
     ]
    }
   ],
   "source": [
    "overall_rate = y_final.mean()\n",
    "top_decile_rate = cal_summary.iloc[-1][\"event_rate\"]\n",
    "lift = top_decile_rate / overall_rate\n",
    "\n",
    "print(\"Overall default rate:\", overall_rate)\n",
    "print(\"Top decile default rate:\", top_decile_rate)\n",
    "print(\"Top decile lift:\", lift)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0321d77",
   "metadata": {},
   "source": [
    "# 13) Review-Capacity Scenarios (Top X% flagged)\n",
    "# Instead of selecting threshold via F1, evaluate operational scenarios:\n",
    "# - If we can review top 10%, 20%, 30% of loans by score,\n",
    "#   how much recall do we get and what precision?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d4d910bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target flag rate: 0.1\n",
      "Threshold: 0.642\n",
      "Precision: 0.0693\n",
      "Recall: 0.2968\n",
      "\n",
      "Target flag rate: 0.2\n",
      "Threshold: 0.554\n",
      "Precision: 0.055\n",
      "Recall: 0.4709\n",
      "\n",
      "Target flag rate: 0.3\n",
      "Threshold: 0.481\n",
      "Precision: 0.0471\n",
      "Recall: 0.6052\n"
     ]
    }
   ],
   "source": [
    "for target_flag_rate in [0.10, 0.20, 0.30]:\n",
    "    thresh = np.percentile(proba_final, 100 * (1 - target_flag_rate))\n",
    "    y_pred_tmp = (proba_final >= thresh).astype(int)\n",
    "\n",
    "    recall_tmp = recall_score(y_final, y_pred_tmp)\n",
    "    precision_tmp = precision_score(y_final, y_pred_tmp)\n",
    "\n",
    "    print(f\"\\nTarget flag rate: {target_flag_rate}\")\n",
    "    print(\"Threshold:\", round(thresh, 3))\n",
    "    print(\"Precision:\", round(precision_tmp, 4))\n",
    "    print(\"Recall:\", round(recall_tmp, 4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
